{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80e0021-7705-4957-96ba-e97d85ee5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a976cce-6ca2-4a9d-b914-dd24206f3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Evaluation of trial zero Shot on proposition\n",
      "\n",
      "\n",
      "- accuracy: 0.5569811320754717 +- 0.0012324476693248783 \n",
      "\n",
      "- f1: 0.3589320683507965 +-0.0011220137448347029 \n",
      "\n",
      "- confusion matrix: \n",
      " \n",
      "\t \t pred pos \t pred neg \n",
      "\n",
      "act pos \t 577 \t\t 63 \n",
      " \n",
      "act neg \t 522 \t\t 163 \n",
      " \n"
     ]
    }
   ],
   "source": [
    "ty = \"proposition\" # \"locution\" \"proposition\"\n",
    "\n",
    "f1s = []\n",
    "accs = []\n",
    "\n",
    "for run in range(1,4):\n",
    "\n",
    "    pred_df = pd.read_csv(f\"Llama3-8B_zeroShot_{ty}_preds_{run}.tsv\", sep=\"\\t\")\n",
    "    preds = pred_df[\"pred\"]\n",
    "    gold_labels = [\"non-contradiction\" if label==0 else \"self-contradiction\" for label in pred_df[\"gold_label\"]]\n",
    "    pred_df[\"gold_label\"] = gold_labels\n",
    "    \n",
    "    acc = accuracy_score(gold_labels, preds)\n",
    "    f1 = f1_score([1 if lab == \"self-contradiction\" else 0 for lab in gold_labels], [1 if lab == \"self-contradiction\" else 0 for lab in preds])\n",
    "    conf_matrix = confusion_matrix(gold_labels, preds)\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "\n",
    "print(f\"---Evaluation of trial zero Shot on {ty}\\n\\n\")\n",
    "print(f\"- accuracy: {mean(accs)} +- {np.std(np.asarray(accs))} \\n\")\n",
    "print(f\"- f1: {mean(f1s)} +-{np.std(np.asarray(f1s))} \\n\")\n",
    "print(f\"- confusion matrix: \\n \")\n",
    "print(\"\\t \\t pred pos \\t pred neg \\n\")\n",
    "print(f\"act pos \\t {conf_matrix[0][0]} \\t\\t {conf_matrix[0][1]} \\n \")\n",
    "print(f\"act neg \\t {conf_matrix[1][0]} \\t\\t {conf_matrix[1][1]} \\n \")\n",
    "\n",
    "#with open(f\"evaluation_GPT_zeroShot_{ty}_2\", \"w\") as fout:\n",
    "#    fout.write(f\"---Evaluation of zero shot trial \\n\\n\")\n",
    "    \n",
    " #   fout.write(f\"- accuracy: {acc} +- {np.std(np.asarray(accs))}\\n\")\n",
    " #   fout.write(f\"- F1: {f1} +- +-{np.std(np.asarray(f1s))}\\n\\n\")\n",
    "\n",
    "    #fout.write(\"---Confusion matrix---\\n\")\n",
    "    #fout.write(\"\\t \\t pred pos \\t pred neg \\n\")\n",
    "    #fout.write(f\"act pos \\t {conf_matrix[0][0]} \\t\\t {conf_matrix[0][1]} \\n \")\n",
    "    #fout.write(f\"act neg \\t {conf_matrix[1][0]} \\t\\t {conf_matrix[1][1]} \\n \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6d8feb-1b59-411c-9ca5-32b52fddf679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(5.551115123125783e-17,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c08c30-d02a-4047-8fd7-791da27f7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prime_df = pd.read_csv(f\"primes_{num}_{seed}.tsv\", sep=\"\\t\")\n",
    "#prime_labels = list(prime_df[\"label\"])\n",
    "\n",
    "    for i in range(len(preds)):\n",
    "        pred = preds[i]\n",
    "        if len(pred.split()) > 1 and \"non-contradiction\" in pred: #and pred.lower().replace(\"'\", \"\").replace(\".\", \"\").endswith(\"non-contradiction\"):\n",
    "            #print(pred)\n",
    "            preds[i] = \"non-contradiction\"\n",
    "            \n",
    "        elif len(pred.split()) > 1 and pred.replace(\"'\", \"\").replace(\".\", \"\").endswith(\"self-contradiction\"):\n",
    "            preds[i] = \"self-contradiction\"\n",
    "\n",
    "        elif \"self-contradiction\" in pred:\n",
    "            preds[i] = \"self-contradiction\"\n",
    "\n",
    "        elif \"non-contradictory\" in pred:\n",
    "            preds[i] = \"non-contradiction\"\n",
    "\n",
    "        elif pred == \"contradiction\":\n",
    "            preds[i] = \"self-contradiction\"\n",
    "\n",
    "        elif pred.split()[-1] == \"contradiction\":\n",
    "            preds[i] = \"self-contradiction\"\n",
    "\n",
    "        elif \"self-contradictory\" in pred:\n",
    "            preds[i] = \"self-contradiction\"\n",
    "\n",
    "    pred_df[\"prediction\"] = preds\n",
    "\n",
    "    print(set(preds))\n",
    "    correct = []\n",
    "\n",
    "    for pred, label in zip(preds, gold_labels):\n",
    "        if pred == label:\n",
    "            correct.append(True)\n",
    "        else:\n",
    "            correct.append(False)\n",
    "\n",
    "    pred_df[\"correct\"] = correct\n",
    "\n",
    "\n",
    "    acc = accuracy_score(gold_labels, preds)\n",
    "    f1 = f1_score([1 if lab == \"self-contradiction\" else 0 for lab in gold_labels], [1 if lab == \"self-contradiction\" else 0 for lab in preds])\n",
    "    conf_matrix = confusion_matrix(gold_labels, preds)\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)\n",
    "\n",
    "print(f\"---Evaluation of trial zero Shot on {ty}\\n\\n\")\n",
    "print(f\"- accuracy: {mean(accs)} +- {np.std(np.asarray(accs))} \\n\")\n",
    "print(f\"- f1: {mean(f1s)} +-{np.std(np.asarray(f1s))} \\n\")\n",
    "print(f\"- confusion matrix: \\n \")\n",
    "print(\"\\t \\t pred pos \\t pred neg \\n\")\n",
    "print(f\"act pos \\t {conf_matrix[0][0]} \\t\\t {conf_matrix[0][1]} \\n \")\n",
    "print(f\"act neg \\t {conf_matrix[1][0]} \\t\\t {conf_matrix[1][1]} \\n \")\n",
    "\n",
    "with open(f\"evaluation_GPT_zeroShot_{ty}_2\", \"w\") as fout:\n",
    "    fout.write(f\"---Evaluation of zero shot trial \\n\\n\")\n",
    "    \n",
    "    fout.write(f\"- accuracy: {acc} +- {np.std(np.asarray(accs))}\\n\")\n",
    "    fout.write(f\"- F1: {f1} +- +-{np.std(np.asarray(f1s))}\\n\\n\")\n",
    "\n",
    "    fout.write(\"---Confusion matrix---\\n\")\n",
    "    fout.write(\"\\t \\t pred pos \\t pred neg \\n\")\n",
    "    fout.write(f\"act pos \\t {conf_matrix[0][0]} \\t\\t {conf_matrix[0][1]} \\n \")\n",
    "    fout.write(f\"act neg \\t {conf_matrix[1][0]} \\t\\t {conf_matrix[1][1]} \\n \")\n",
    "#print(pred_df)\n",
    "#pred_df.to_csv(f\"predictions_{num}_{seed}_clean_eval.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
