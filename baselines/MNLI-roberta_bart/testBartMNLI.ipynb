{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/henrike/.cache/torch/hub/pytorch_fairseq_main\n",
      "2024-04-07 11:34:48 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/bart.large.mnli.tar.gz from cache at /home/henrike/.cache/torch/pytorch_fairseq/5d221d78c74e1b8f4cad8f0e81927bd91d96f2f5da9a2547475e6212f5da3b89.4f1c48b34c2db97b55c12d62dace1b8776cfff21c8784b57a8047ff0f6389435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to build Cython components. Please make sure Cython is installed if the torch.hub model you are loading depends on it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henrike/my-python3.8-mnli-modeltest/lib/python3.8/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/home/henrike/my-python3.8-mnli-modeltest/lib/python3.8/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/home/henrike/my-python3.8-mnli-modeltest/lib/python3.8/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/home/henrike/my-python3.8-mnli-modeltest/lib/python3.8/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/home/henrike/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/home/henrike/my-python3.8-mnli-modeltest/lib/python3.8/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/home/henrike/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/bart/model.py:127: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "2024-04-07 11:34:50 | INFO | fairseq.tasks.denoising | dictionary: 50264 types\n",
      "2024-04-07 11:35:08 | INFO | fairseq.models.bart.model | Registering classification head: mnli\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BARTHubInterface(\n",
       "  (models): ModuleList(\n",
       "    (0): BARTModel(\n",
       "      (encoder): TransformerEncoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (decoder): TransformerDecoderBase(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TransformerDecoderLayerBase(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "      )\n",
       "      (classification_heads): ModuleDict(\n",
       "        (mnli): BARTClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): BARTModel(\n",
       "    (encoder): TransformerEncoderBase(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerEncoderLayerBase(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoderBase(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x TransformerDecoderLayerBase(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_dropout_module): FairseqDropout()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (mnli): BARTClassificationHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart = torch.hub.load('pytorch/fairseq', 'bart.large.mnli')\n",
    "bart.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../../ProSeCCo/fullCorpus/ProSeCCo_final.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "locutions1 = list(dataset[\"locution1\"])\n",
    "locutions2 = list(dataset[\"locution2\"])\n",
    "\n",
    "propositions1 = list(dataset[\"proposition1\"])\n",
    "propositions2 = list(dataset[\"proposition2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loc_preds = []\n",
    "\n",
    "for i in range(3):\n",
    "    locution_preds = []\n",
    "\n",
    "    for loc1, loc2 in zip(locutions1, locutions2):\n",
    "        tokens = bart.encode(loc1, loc2)\n",
    "        result = bart.predict(\"mnli\", tokens).argmax()\n",
    "        locution_preds.append(int(result))\n",
    "\n",
    "    dataset[\"bart_mnli_locution_pred\"] = locution_preds\n",
    "    dataset.to_csv(f\"bart-large-mnli-results/bart-large-mnli_locution-predictions_{i+1}.tsv\", sep=\"\\t\", index=False)\n",
    "    all_loc_preds.append(locution_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prop_preds = []\n",
    "\n",
    "for i in range(3):\n",
    "    proposition_preds = []\n",
    "\n",
    "    for prop1, prop2 in zip(propositions1, propositions2):\n",
    "        tokens = bart.encode(prop1, prop2)\n",
    "        result = bart.predict(\"mnli\", tokens).argmax()\n",
    "        proposition_preds.append(int(result))\n",
    "\n",
    "    dataset[\"bart_mnli_proposition_pred\"] = proposition_preds\n",
    "    dataset.to_csv(f\"bart-large-mnli-results/bart-large-mnli_proposition-predictions_{i+1}.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    all_prop_preds.append(proposition_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"bart_mnli_proposition_pred\"] = proposition_preds\n",
    "dataset.to_csv(\"bart-large-mnli-results/bart-large-mnli_proposition-predictions_2.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_accs = []\n",
    "loc_f1s = []\n",
    "\n",
    "prop_accs = []\n",
    "prop_f1s = []\n",
    "\n",
    "for i in range(1,4):\n",
    "    locution_dataset = pd.read_csv(f\"bart-large-mnli-results/bart-large-mnli_locution-predictions_{i}.tsv\", sep=\"\\t\")\n",
    "    proposition_dataset = pd.read_csv(f\"bart-large-mnli-results/bart-large-mnli_proposition-predictions_{i}.tsv\", sep=\"\\t\")\n",
    "\n",
    "    loc_preds = [1 if lab==0 else 0 for lab in locution_dataset[\"bart_mnli_locution_pred\"]]\n",
    "    loc_gold = [1 if lab==\"contradiction\" else 0 for lab in locution_dataset[\"label\"]]\n",
    "\n",
    "    prop_preds = [1 if lab==0 else 0 for lab in proposition_dataset[\"bart_mnli_proposition_pred\"]]\n",
    "    prop_gold = [1 if lab==\"contradiction\" else 0 for lab in proposition_dataset[\"label\"]]\n",
    "\n",
    "    loc_accs.append(accuracy_score(loc_gold, loc_preds))\n",
    "    loc_f1s.append(f1_score(loc_gold, loc_preds))\n",
    "    \n",
    "    prop_accs.append(accuracy_score(prop_gold, prop_preds))\n",
    "    prop_f1s.append(f1_score(prop_gold, prop_preds))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5381132075471698, 0.5381132075471698, 0.5381132075471698]\n"
     ]
    }
   ],
   "source": [
    "print(loc_accs)\n",
    "#prop_cm = confusion_matrix(prop_gold, prop_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results with locutions: accuracy: 0.5381132075471698 +- 0.0 \n",
      " f1: 0.4354243542435424 +- 5.551115123125783e-17\n",
      "results with propositions: accuracy: 0.550188679245283 +- 0.0 \n",
      " f1: 0.4649910233393178 +- 0.0\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "print(\"results with locutions:\",\n",
    "      \"accuracy:\", mean(loc_accs), \"+-\", np.std(np.asarray(loc_accs)), \"\\n\",\n",
    "      \"f1:\", mean(loc_f1s), \"+-\", np.std(np.asarray(loc_f1s))\n",
    ")\n",
    "\n",
    "\n",
    "print(\"results with propositions:\",\n",
    "      \"accuracy:\", mean(prop_accs), \"+-\", np.std(np.asarray(prop_accs)), \"\\n\",\n",
    "      \"f1:\", mean(prop_f1s), \"+-\", np.std(np.asarray(prop_f1s))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8-mnlitest-kernel",
   "language": "python",
   "name": "python3.8-mnlitest-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
